{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n6ecm6wEcwgn"
   },
   "source": [
    "# Detect Body Points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h4C9zQgLc0WG"
   },
   "source": [
    "## Import Libraires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "18H4o1Whc1ye"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from google.colab.patches import cv2_imshow\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2MjOn5rmdBbU"
   },
   "source": [
    "## Load Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TF55mtemdCy6",
    "outputId": "43203e13-784b-493b-a458-7ece9e592466"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "7TmIEiZNdIQC",
    "outputId": "63b07e7e-f800-4099-8eb6-5398acb66eb7"
   },
   "outputs": [],
   "source": [
    "image = cv2.imread('/content/drive/MyDrive/Computer Vision Masterclass/Images/megan.jpg')\n",
    "cv2_imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iduGveg9daHe",
    "outputId": "550c2b9f-330a-4147-a325-adfd1294cff5"
   },
   "outputs": [],
   "source": [
    "image.shape, image.shape[0]*image.shape[1]*image.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YpuVZ5VcdeOt",
    "outputId": "b0da85c2-7dc3-46be-cebd-35cf45d9bc60"
   },
   "outputs": [],
   "source": [
    "#change order of image.shape to send to NN\n",
    "image_blob = cv2.dnn.blobFromImage(image=image, scalefactor=1.0/255, size=(image.shape[1], image.shape[0])) #normalise also\n",
    "image_blob.shape\n",
    "\n",
    "#batchsize, #channels, #dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xcy17CfHgKkR"
   },
   "source": [
    "## Load pre-trained network\n",
    "- Caffe Deep Learning Framework : https://caffe.berkeleyvision.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h6SJIbGUdfA1"
   },
   "outputs": [],
   "source": [
    "#openCv integrated with Caffe\n",
    "network = cv2.dnn.readNetFromCaffe('/content/drive/MyDrive/Computer Vision Masterclass/Weights/pose_deploy_linevec_faster_4_stages.prototxt',  #path\n",
    "                                   '/content/drive/MyDrive/Computer Vision Masterclass/Weights/pose_iter_160000.caffemodel')  #weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0okalnJedfEb",
    "outputId": "68bfc55e-f463-4225-af7f-4106235a0335"
   },
   "outputs": [],
   "source": [
    "network.getLayerNames()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oxXRrWB8dfHb",
    "outputId": "814615e2-b997-42a1-fcec-128e3e437a09"
   },
   "outputs": [],
   "source": [
    "len(network.getLayerNames())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "po0ryFj5hqdG"
   },
   "source": [
    "## Predict Body Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oMze7zEodfKM"
   },
   "outputs": [],
   "source": [
    "network.setInput(image_blob)\n",
    "output = network.forward() #image sent to input layer of NN and output at end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_ekfI-9ZdfNm",
    "outputId": "5bcacd8c-2aae-4473-8320-c43493bd5b6a"
   },
   "outputs": [],
   "source": [
    "output.shape #batchsize, confidences, locations of points in image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-VUbkBm8dfQc"
   },
   "outputs": [],
   "source": [
    "position_width = output.shape[3]\n",
    "position_height = output.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uy64Qh9CdfU7",
    "outputId": "40b02be9-06ee-47e7-d654-8970376d0b55"
   },
   "outputs": [],
   "source": [
    "num_points = 15 #totally 0-15 points from this NN but here we're not considering point15- background\n",
    "points =[] #locations points in images\n",
    "threshold = 0.1 #only return points where confidence is higher than 10%\n",
    "\n",
    "for i in range(num_points):\n",
    "  confidence_map = output[0, i, :, :]# 0-first image; i-contains info about detected points\n",
    "  #each of the 14 point contains a vector of size 43 which represents confidence levels. So max confidence is considered\n",
    "  _, confidence, _, point = cv2.minMaxLoc(confidence_map) #get max confidence value and the point\n",
    "\n",
    "  #returned coordinates are to be scaled with respect to original image\n",
    "  x = int((image.shape[1] * point[0]) / position_width)\n",
    "  y = int((image.shape[0] * point[1]) / position_height)\n",
    "\n",
    "  if(confidence > threshold):\n",
    "    cv2.circle(image, (x, y), 2, (0, 0, 255), thickness = 2)\n",
    "    cv2.putText(image, '{}'.format(i), (x, y), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), thickness=2)\n",
    "    points.append((x,y))\n",
    "    print(\"Point :\", i, \"\\nConfidence :\", confidence, \"\\nLocation :\", (x,y), \"\\n\")\n",
    "  else:\n",
    "    points.append(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WCn9vIArqttq"
   },
   "source": [
    "## Display Image with Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "bi77GPVklkXj",
    "outputId": "dfc7179b-70f1-4e1c-fa3c-9b8fdd5a5dde"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 20))\n",
    "plt.axis('off')\n",
    "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zLXdxVoit0gJ"
   },
   "source": [
    "## Draw Lines Connecting Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EbJYiX-9lWHF",
    "outputId": "a0dc7758-3115-421a-a9d0-4d25aceabcdd"
   },
   "outputs": [],
   "source": [
    "point_connections =[[0,1], [1,2], [2,3], [3,4], [1,5], [5,6], [6,7], [1,14], [14,8], [8,9], [9,10], [14,11], [11,12], [12,13]]\n",
    "point_connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ANvzYRCZdfiF"
   },
   "outputs": [],
   "source": [
    "for connection in point_connections:\n",
    "  pointA = connection[0]\n",
    "  pointB = connection[1]\n",
    "  if(points[pointA] and points[pointB]):  #if connection exist\n",
    "    cv2.line(image, points[pointA], points[pointB], (255,0,0), 1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 658
    },
    "id": "eqgtvHiTcGwd",
    "outputId": "1476fee9-7abc-4e6f-c2e9-192338b69661"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 20))\n",
    "plt.axis('off')\n",
    "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ODvRWZdZcmuG"
   },
   "source": [
    "# Detect Movements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zsgVmKlMeQy9"
   },
   "source": [
    "## Arms Above Head (Image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 589
    },
    "id": "c0zr5l8lcqPH",
    "outputId": "b2fa57af-e0f8-4597-8aa6-0b029f8ee631"
   },
   "outputs": [],
   "source": [
    "image2 = cv2.imread('/content/drive/MyDrive/Computer Vision Masterclass/Images/player.jpg')\n",
    "image_blob2 = cv2.dnn.blobFromImage(image=image2, scalefactor=1.0/255, size=(image2.shape[1], image2.shape[0])) #normalise also\n",
    "\n",
    "network.setInput(image_blob2)\n",
    "output2 = network.forward() #image sent to input layer of NN and output at end\n",
    "\n",
    "position_width = output2.shape[3]\n",
    "position_height = output2.shape[2]\n",
    "\n",
    "num_points = 15 #totally 0-15 points from this NN but here we're not considering point15- background\n",
    "points =[] #locations points in images\n",
    "threshold = 0.1 #only return points where confidence is higher than 10%\n",
    "\n",
    "for i in range(num_points):\n",
    "  confidence_map = output2[0, i, :, :]# 0-first image; i-contains info about detected points\n",
    "  #each of the 14 point contains a vector of size 43 which represents confidence levels. So max confidence is considered\n",
    "  _, confidence, _, point = cv2.minMaxLoc(confidence_map) #get max confidence value and the point\n",
    "\n",
    "  #returned coordinates are to be scaled with respect to original image\n",
    "  x = int((image2.shape[1] * point[0]) / position_width)\n",
    "  y = int((image2.shape[0] * point[1]) / position_height)\n",
    "\n",
    "  if(confidence > threshold):\n",
    "    cv2.circle(image2, (x, y), 2, (0, 0, 255), thickness = 2)\n",
    "    cv2.putText(image2, '{}'.format(i), (x, y), cv2.FONT_HERSHEY_SIMPLEX, .3, (0, 255, 0))\n",
    "    cv2.putText(image2, '{}-{}'.format(point[0], point[1]), (x, y+10), cv2.FONT_HERSHEY_SIMPLEX, .5, (255, 0, 0))\n",
    "    points.append((x,y))\n",
    "    #print(\"Point :\", i, \"\\nConfidence :\", confidence, \"\\nLocation :\", (x,y), \"\\n\")\n",
    "  else:\n",
    "    points.append(None)\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.axis('off')\n",
    "plt.imshow(cv2.cvtColor(image2, cv2.COLOR_BGR2RGB));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "77esIrEldflD"
   },
   "outputs": [],
   "source": [
    "def verify_arms_up(points):\n",
    "  head, right_wrist, left_wrist = 0, 0, 0\n",
    "  for i, point in enumerate(points):\n",
    "    if(i==0):\n",
    "      head = point[1]\n",
    "    elif (i==4):\n",
    "      right_wrist = point[1]\n",
    "    elif (i==7):\n",
    "      left_wrist = point[1]\n",
    "  #print(head, right_wrist, left_wrist)\n",
    "\n",
    "  if(right_wrist<head and left_wrist<head):\n",
    "    return True\n",
    "  else:\n",
    "    return False\n",
    "\n",
    "verify_arms_up(points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OWx9y27olYOM"
   },
   "source": [
    "## Arms Above Head (Video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VCiJUAGYlc_9",
    "outputId": "bb79717c-a374-48f4-999f-9bb039d150fc"
   },
   "outputs": [],
   "source": [
    "video = '/content/drive/MyDrive/Computer Vision Masterclass/Videos/gesture1.mp4'\n",
    "capture = cv2.VideoCapture(video)\n",
    "status, frame = capture.read()\n",
    "status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m1WggBphldH1"
   },
   "outputs": [],
   "source": [
    "result = '/content/drive/MyDrive/Computer Vision Masterclass/Videos/gesture1_result.mp4'\n",
    "save_video = cv2.VideoWriter(result, cv2.VideoWriter_fourcc(*'XVID'), 10, (frame.shape[1], frame.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "UjCsYliVldKe",
    "outputId": "6687c08c-f51c-4caa-9df0-51a077465e95"
   },
   "outputs": [],
   "source": [
    "threshold = 0.1\n",
    "while cv2.waitKey(1) < 0:\n",
    "  status, frame = capture.read()\n",
    "\n",
    "  if(not status):\n",
    "    break\n",
    "\n",
    "  image_blob = cv2.dnn.blobFromImage(image=frame, scalefactor=1.0/255, size=(256, 256)) #normalise also\n",
    "  network.setInput(image_blob)\n",
    "  output = network.forward() #image sent to input layer of NN and output at end\n",
    "\n",
    "  position_width = output.shape[3]\n",
    "  position_height = output.shape[2]\n",
    "\n",
    "  num_points = 15 #totally 0-15 points from this NN but here we're not considering point15- background\n",
    "  points =[] #locations points in images\n",
    "  for i in range(num_points):\n",
    "    confidence_map = output[0, i, :, :]# 0-first image; i-contains info about detected points\n",
    "    #each of the 14 point contains a vector of size 43 which represents confidence levels. So max confidence is considered\n",
    "    _, confidence, _, point = cv2.minMaxLoc(confidence_map) #get max confidence value and the point\n",
    "\n",
    "    #returned coordinates are to be scaled with respect to original image\n",
    "    x = int((frame.shape[1] * point[0]) / position_width)\n",
    "    y = int((frame.shape[0] * point[1]) / position_height)\n",
    "\n",
    "    if(confidence > threshold):\n",
    "      cv2.circle(frame, (x, y), 2, (0, 0, 255), thickness = 2)\n",
    "      cv2.putText(frame, '{}'.format(i), (x, y), cv2.FONT_HERSHEY_SIMPLEX, .3, (0, 255, 0))\n",
    "      cv2.putText(frame, '{}-{}'.format(point[0], point[1]), (x, y+10), cv2.FONT_HERSHEY_SIMPLEX, .5, (255, 0, 0))\n",
    "      points.append((x,y))\n",
    "      #print(\"Point :\", i, \"\\nConfidence :\", confidence, \"\\nLocation :\", (x,y), \"\\n\")\n",
    "    else:\n",
    "      points.append(None)\n",
    "\n",
    "  for connection in point_connections:\n",
    "    pointA = connection[0]\n",
    "    pointB = connection[1]\n",
    "    if(points[pointA] and points[pointB]):  #if connection exist\n",
    "      cv2.line(frame, points[pointA], points[pointB], (255,0,0), 1)\n",
    "\n",
    "  if(verify_arms_up(points)==True):\n",
    "    cv2.putText(frame, 'Exercise Done', (50,200), cv2.FONT_HERSHEY_COMPLEX, 3, (0,0,255))\n",
    "\n",
    "  cv2_imshow(frame)\n",
    "  save_video.write(frame)\n",
    "save_video.release()   "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "h4C9zQgLc0WG",
    "2MjOn5rmdBbU",
    "xcy17CfHgKkR",
    "po0ryFj5hqdG",
    "WCn9vIArqttq",
    "zLXdxVoit0gJ",
    "zsgVmKlMeQy9"
   ],
   "name": "Gesture and Action Recognition",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
