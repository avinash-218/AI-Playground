{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LBPH-Face Recognition1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLzTRpC6qhYs"
      },
      "source": [
        "#import libraries\n",
        "from PIL import Image #work with images\n",
        "import cv2\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow\n",
        "import zipfile\n",
        "import os\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import seaborn\n",
        "\n",
        "#path of the dataset zip file\n",
        "path=\"/content/drive/MyDrive/Computer Vision Masterclass/Datasets/yalefaces.zip\"\n",
        "\n",
        "#uncompress the file\n",
        "zip_obj = zipfile.ZipFile(file=path, mode='r')  #read mode\n",
        "zip_obj.extractall('./')  #extract in current folder\n",
        "zip_obj.close() #release memory"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hHd1uzpt9iw"
      },
      "source": [
        "#get np_representatin of image and class labels\n",
        "def get_image_data(): #return id(class) and numpy representation of each image\n",
        "  paths=[os.path.join('/content/yalefaces/train', f) for f in os.listdir('/content/yalefaces/train')] #path of train data\n",
        "  #print(paths)\n",
        "  faces=[]\n",
        "  ids=[]\n",
        "  for path in paths:\n",
        "    #print(path)\n",
        "    \n",
        "    #get numpy representation of images\n",
        "    image = Image.open(path).convert('L')   #L mode means gray scale mode (L- luminous)\n",
        "    image_np = np.array(image, 'uint8')     #numpy representation of image\n",
        "    faces.append(image_np)\n",
        "\n",
        "    #get classes\n",
        "    id = int(os.path.split(path)[1].split('.')[0].replace('subject',''))      #extracting class names by python string manipulations\n",
        "    #print(id)\n",
        "    ids.append(id)\n",
        "  return np.array(ids), faces\n",
        "    \n",
        "ids, faces = get_image_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4NuvgNly1O2"
      },
      "source": [
        "#default parameters\n",
        "#radius :1\n",
        "#neighbors: 8\n",
        "#grid_x: 8\n",
        "#grid_y: 8\n",
        "#threshold: 1.7976931348623157e+308\n",
        "\n",
        "#training\n",
        "lbph_classifier = cv2.face.LBPHFaceRecognizer_create(radius=4, neighbors=14, grid_x=9, grid_y=9)\n",
        "lbph_classifier.train(faces, ids)\n",
        "lbph_classifier.write('lbph_classifier.yml')    #save file- contains all histograms for each image\n",
        "\n",
        "#load saved file\n",
        "lbph_face_classifier = cv2.face.LBPHFaceRecognizer_create()\n",
        "lbph_face_classifier.read('/content/lbph_classifier.yml') #loading the saved file\n",
        "\n",
        "#evaluation\n",
        "paths=[os.path.join('/content/yalefaces/test', f) for f in os.listdir('/content/yalefaces/test')]   #path of test data\n",
        "predictions = []\n",
        "expected_outs = []\n",
        "for path in paths:\n",
        "  image = Image.open(path).convert('L')   #L mode means gray scale mode (L- luminous)\n",
        "  image_np = np.array(image, 'uint8')\n",
        "  prediction, _ = lbph_face_classifier.predict(image_np)   #second return value is confidence level\n",
        "  expected_out = int(os.path.split(path)[1].split('.')[0].replace('subject',''))  #extract class name\n",
        "\n",
        "  #writing over the image about predicted and expected classes\n",
        "  cv2.putText(image_np, 'Pred: '+str(prediction), (10,30), cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, (0,255,0))\n",
        "  cv2.putText(image_np, 'Exp: ' +str(expected_out),  (10,50), cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, (0,255,0))\n",
        "  cv2_imshow(image_np)  #only grayscale image possible\n",
        "\n",
        "  predictions.append(prediction)\n",
        "  expected_outs.append(expected_out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_u75nlbB2nG6"
      },
      "source": [
        "predictions = np.array(predictions)\n",
        "expected_outs = np.array(expected_outs)\n",
        "print('Accuracy :',accuracy_score(predictions, expected_outs)*100,'%',end='\\n\\n') #acuracy of predictions on test data\n",
        "\n",
        "#confusion matrix\n",
        "cm = confusion_matrix(predictions, expected_outs)\n",
        "\n",
        "#heatmap visualisation\n",
        "seaborn.heatmap(cm, annot=True);  #semicolon to hide message from matplotlib; annot - to see values"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}